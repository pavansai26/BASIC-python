{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyOQqza9tPpcQvbvjri4eVn+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/pavansai26/BASIC-python/blob/master/LLMS_WITH_MLFLOW.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Install MLflow and transformers: Transformers and MLflow installation can be done using Pip.\n"
      ],
      "metadata": {
        "id": "OXOmyGk987Sh"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZUkvWMD38zG4"
      },
      "outputs": [],
      "source": [
        "!pip install transformers -q\n",
        "!pip install mlflow -q\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define your LLM: The transformers library can be used to define your LLM, as shown in the following Python code:"
      ],
      "metadata": {
        "id": "LHicc-7m9ZWv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import transformers\n",
        "import mlflow\n",
        "\n",
        "chat_pipeline = transformers.pipeline(model=\"microsoft/DialoGPT-medium\")"
      ],
      "metadata": {
        "id": "h7xHN9o-8_mY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Log your LLM: To log your LLM to MLflow, use the Python code snippet below:\n"
      ],
      "metadata": {
        "id": "uDLRpqH99csg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with mlflow.start_run():\n",
        "  model_info = mlflow.transformers.log_model(\n",
        "    transformers_model=chat_pipeline,\n",
        "    artifact_path=\"chatbot\",\n",
        "    input_example=\"Hi there!\"\n",
        "  )"
      ],
      "metadata": {
        "id": "gocaTwMa9RzQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load your LLM and make predictions from it:\n"
      ],
      "metadata": {
        "id": "Jds9XJv3-Dwo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load as interactive pyfunc\n",
        "chatbot = mlflow.pyfunc.load_model(model_info.model_uri)\n",
        "#make predictions\n",
        "chatbot.predict(\"What is the best way to get to Antarctica?\")\n",
        "#'I think you can get there by boat'\n",
        "chatbot.predict(\"What kind of boat should I use?\")\n",
        "#'A boat that can go to Antarctica.'"
      ],
      "metadata": {
        "id": "maghyA67-DFS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "aagd4lnC_yr7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install openai\n",
        "!pip install mlflow"
      ],
      "metadata": {
        "id": "Gc7ayaJw_yQc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List\n",
        "import openai\n",
        "import mlflow"
      ],
      "metadata": {
        "id": "Us8LO8-k_0mW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def chat_completion(inputs: List[str]) -> List[str]:\n",
        "    # Model signature is automatically constructed from\n",
        "    # type annotations. The signature for this model\n",
        "    # would look like this:\n",
        "    # ----------\n",
        "    # signature:\n",
        "    #   inputs: [{\"type\": \"string\"}]\n",
        "    #   outputs: [{\"type\": \"string\"}]\n",
        "    # ----------\n",
        "\n",
        "    outputs = []\n",
        "\n",
        "    for input in inputs:\n",
        "        completion = openai.ChatCompletion.create(\n",
        "            model=\"gpt-3.5-turbo\",\n",
        "            messages=[{\"role\": \"user\", \"content\": \"<prompt>\"}]\n",
        "        )\n",
        "\n",
        "        outputs.append(completion.choices[0].message.content)\n",
        "\n",
        "    return outputs"
      ],
      "metadata": {
        "id": "c29G7JgtAL4S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Log the model\n",
        "mlflow.pyfunc.log_model(\n",
        "    artifact_path=\"model\",\n",
        "    python_model=chat_completion,\n",
        "    pip_requirements=[\"openai\"],\n",
        ")"
      ],
      "metadata": {
        "id": "TaK9f7jrAYYu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}